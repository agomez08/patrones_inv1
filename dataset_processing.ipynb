{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deluxe-light",
   "metadata": {},
   "source": [
    "# Dataset Pre-Processing\n",
    "\n",
    "In this notebook, we present a series of steps to illustrate the process of cleaning out a dataset before using it for a learnng task related to patterns-recognition.\n",
    "\n",
    "For this purpose, we are going to use the **1.6 million UK traffic accidents** dataset that can be downloaded from the next link: https://www.kaggle.com/daveianhickey/2000-16-traffic-flow-england-scotland-wales\n",
    "\n",
    "From its documentation, this dataset contains the most relevant information for police reported accidents in the UK between 2005 and 2014, although 2008 is missing. Each of the 1.6 million accidents reported in this dataset is described with 33 features (33 columns for each instance).\n",
    "\n",
    "We will attempt to visualize and understand the most relevant contents of the dataset while implementing improvements that can make it more suitable for a learning task. It is worth mentioning that the specific type of pre-processing to apply depends on the application and type of model we desire to implement. We will comment on certain actions that could be taken but that are not necessarily applied to keep the data somehow generic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-liberal",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing relevant python modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the 3 datasets and merge into a single data-frame\n",
    "# Note: Specifying type for one of the columns since it seems to have mixed datatypes\n",
    "df_05_07 = pd.read_csv('dataset/raw/accidents_2005_to_2007.csv', dtype={'LSOA_of_Accident_Location': 'string'})\n",
    "df_09_11 = pd.read_csv('dataset/raw/accidents_2009_to_2011.csv', dtype={'LSOA_of_Accident_Location': 'string'})\n",
    "df_12_14 = pd.read_csv('dataset/raw/accidents_2012_to_2014.csv', dtype={'LSOA_of_Accident_Location': 'string'})\n",
    "df_accidents_05_14 = pd.concat([df_05_07, df_09_11, df_12_14], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm values have been concatenated vertically by printing the DF size\n",
    "print(\"df_accidents_05_14 shape is: {}\".format(df_accidents_05_14.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-liverpool",
   "metadata": {},
   "source": [
    "From the printed text above, We can observe how the whole raw dataset has a little more than 1.5 million instances and not quite 1.6 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print head to visualize initial column values\n",
    "df_accidents_05_14.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-ribbon",
   "metadata": {},
   "source": [
    "## Removing Replicated Instances\n",
    "Even though the instances are supposed to be unique, some of them were actually replicated from file to file.\n",
    "See this: https://www.kaggle.com/daveianhickey/2000-16-traffic-flow-england-scotland-wales/discussion/51835\n",
    "\n",
    "We can confirm this by printing the count for the most repeated values in the *'Accident_Index'* column. As it can be seen below, there is a chance that the same accidents are being replicated multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 10 most frequent IDs and their count\n",
    "print(df_accidents_05_14['Accident_Index'].value_counts()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-anchor",
   "metadata": {},
   "source": [
    "As recommended in the discussion linked above, we will remove the replicated items when they have the same value for multiple properties. This because there is a disclaimer on the page of the dataset indicating that the *'Accident_Index'* property may not be formatted correctly in all the cases and we should not trust it 100% as unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_vars=['Accident_Index','Date','LSOA_of_Accident_Location','Time','Longitude','Latitude']\n",
    "df_accidents_05_14 = df_accidents_05_14.drop_duplicates(subset=relevant_vars, keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-county",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the new shape\n",
    "print(\"df_accidents_05_14 new shape is: {}\".format(df_accidents_05_14.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-pressing",
   "metadata": {},
   "source": [
    "### Removing *'Accident_Index'* Column\n",
    "As it was discussed above, we cannot trust this column to be formatted correctly and be unique for each of the reported accidents. Besides, since this is an identifier, it would not offer any valuable insight for a training algorithm. We will go ahead and remove it then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Accident_Index' column\n",
    "df_accidents_05_14 = df_accidents_05_14.drop(columns=['Accident_Index'])\n",
    "# Print the new shape and head\n",
    "print(\"df_accidents_05_14 new shape is: {}\".format(df_accidents_05_14.shape))\n",
    "df_accidents_05_14.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-approach",
   "metadata": {},
   "source": [
    "## Reviewing Columns Data\n",
    "We will now review the data provided for each of the columns and identify improvements before using them on a training model. We will attempt to visualize the data, identify its usefulness, detect missing elements, identify outliers, group or join elements, transform, normalize, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-malpractice",
   "metadata": {},
   "source": [
    "### Geo-Localization Columns\n",
    "The first 4 columns contain localization information for the accident. The first 2 columns (*'Location_Easting_OSGR'* and *'Location_Northing_OSGR'*) can define a unique location by themselves and are specific for the UK. Considering this, we can identify the *'Longitude'* and *'Latitude'* columns as redundant and we can apply a dimmensionality reduction by removing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Longitude', 'Latitude' columns\n",
    "df_accidents_05_14 = df_accidents_05_14.drop(columns=['Longitude', 'Latitude'])\n",
    "# Print the new shape and head\n",
    "print(\"df_accidents_05_14 new shape is: {}\".format(df_accidents_05_14.shape))\n",
    "df_accidents_05_14.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-claim",
   "metadata": {},
   "source": [
    "As it can be seen below, the two location columns do have missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14[df_accidents_05_14['Location_Easting_OSGR'].isna() | df_accidents_05_14['Location_Northing_OSGR'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-obligation",
   "metadata": {},
   "source": [
    "Since the values represent physical locations, it may not make sense to fill these values with other and we opt for removing these 101 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14.dropna(subset=['Location_Easting_OSGR', 'Location_Northing_OSGR'], inplace=True)\n",
    "print(\"df_accidents_05_14 new shape is: {}\".format(df_accidents_05_14.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-interest",
   "metadata": {},
   "source": [
    "Now let's visualize in a scatter the location columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14.plot.scatter(x='Location_Easting_OSGR', y='Location_Northing_OSGR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-hello",
   "metadata": {},
   "source": [
    "At this point and depending on the application or desired model, we may filter the locations to only include certain regions or areas. However in this case we are going to keep them all. \n",
    "One final step we will make with this location data is reducing their range. These location values are high numbers, while the rest of the columns are not, and also to avoid potential overflow in calculations when implementing a model. For this purpose we will apply a scale of 1e-6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_scale = 1e-6\n",
    "df_accidents_05_14['Location_Easting_OSGR'] = df_accidents_05_14['Location_Easting_OSGR'] * location_scale\n",
    "df_accidents_05_14['Location_Northing_OSGR'] = df_accidents_05_14['Location_Northing_OSGR'] * location_scale\n",
    "# Visualize updated location data\n",
    "df_accidents_05_14.plot.scatter(x='Location_Easting_OSGR', y='Location_Northing_OSGR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-prophet",
   "metadata": {},
   "source": [
    "### *'Police_Force'* Column\n",
    "This column indicates an identifier for the police force responsible of reporting the accident. As we can see below this column does not have missing items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14[df_accidents_05_14['Police_Force'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-undergraduate",
   "metadata": {},
   "source": [
    "From the data visualization below, we can see the data is in an acceptable range and there is not any visible outlier. We will keep this data as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-minutes",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_accidents_05_14.index, df_accidents_05_14['Police_Force'])\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"Police_Force\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-territory",
   "metadata": {},
   "source": [
    "### *'Accident_Severity'* Column\n",
    "This column indicates the severity of the accident in 3 possible values: \n",
    "\n",
    "1- High Severity.\n",
    "\n",
    "2- Mid Severity.\n",
    "\n",
    "3- Low Severity.\n",
    "\n",
    "As we can see below this column does not have missing items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14[df_accidents_05_14['Accident_Severity'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-registration",
   "metadata": {},
   "source": [
    "Since the severity assignments do not correspond with the numerical value, we choose to change the values to follow the next more logical representation:\n",
    "1- Low Severity.\n",
    "\n",
    "2- Mid Severity.\n",
    "\n",
    "3- High Severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-angle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily save 3 as 0\n",
    "df_accidents_05_14.loc[df_accidents_05_14['Accident_Severity'] == 3, 'Accident_Severity'] = 0\n",
    "# Save 1 as 3\n",
    "df_accidents_05_14.loc[df_accidents_05_14['Accident_Severity'] == 1, 'Accident_Severity'] = 3\n",
    "# Save 0 as 1\n",
    "df_accidents_05_14.loc[df_accidents_05_14['Accident_Severity'] == 0, 'Accident_Severity'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-twenty",
   "metadata": {},
   "source": [
    "As we can observe on the histogram below, most of the instances represent low-severity accidents. Depending on the application, we may desire to balance these classes distribution by removing some of the instances of low-severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14.hist(column='Accident_Severity', bins=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-italian",
   "metadata": {},
   "source": [
    "### *'Number_of_Vehicles'* Column\n",
    "This column indicates the number of vehicles involved in the accident. As we can see below this column does not have missing items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14[df_accidents_05_14['Number_of_Vehicles'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-handling",
   "metadata": {},
   "source": [
    "From the data visualization below, we can identify there are a few outliers on the data with not enough instances to be representative for the model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_accidents_05_14.index, df_accidents_05_14['Number_of_Vehicles'])\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"Number_of_Vehicles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-exhaust",
   "metadata": {},
   "source": [
    "Considering the above, we will remove the outlisers that are not within the +-3 standard deviations from mean. With this approach only the values of 1 to 4 are maintained for this column and the number of instances is reduced to 1461460."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the ones that are within +3 to -3 standard deviations (rounded)\n",
    "nv_column = df_accidents_05_14['Number_of_Vehicles']\n",
    "nv_mean = nv_column.mean()\n",
    "nv_std_dev = nv_column.std()\n",
    "print(\"Number_of_Vehicles mean = {}\".format(nv_mean))\n",
    "print(\"Number_of_Vehicles std deviation = {}\".format(nv_std_dev))\n",
    "deviations = np.abs(nv_column - nv_mean)\n",
    "df_accidents_05_14 = df_accidents_05_14[nv_column <= np.round(nv_mean + 3 * nv_std_dev)]\n",
    "\n",
    "# Plot Number_of_Vehicles again\n",
    "plt.scatter(df_accidents_05_14.index, df_accidents_05_14['Number_of_Vehicles'])\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"Number_of_Vehicles Updated\")\n",
    "print(\"df_accidents_05_14 new shape is: {}\".format(df_accidents_05_14.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-cooperative",
   "metadata": {},
   "source": [
    "### *'Number_of_Casualties'* Column\n",
    "This column indicates the number of casualities as a consequence of the accident. As we can see below this column does not have missing items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14[df_accidents_05_14['Number_of_Casualties'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-knight",
   "metadata": {},
   "source": [
    "From the data visualization below and as in the previous case, we can identify there are a few outliers on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_accidents_05_14.index, df_accidents_05_14['Number_of_Casualties'])\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"Number_of_Casualties\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-chamber",
   "metadata": {},
   "source": [
    "Considering the above, we will remove the outlisers that are not within the +-3 standard deviations from mean. With this approach only the values of 1 to 4 are maintained for this column and the number of instances is reduced to 1447801."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the ones that are within +3 to -3 standard deviations (rounded)\n",
    "nc_column = df_accidents_05_14['Number_of_Casualties']\n",
    "nc_mean = nc_column.mean()\n",
    "nc_std_dev = nc_column.std()\n",
    "print(\"Number_of_Casualties mean = {}\".format(nc_mean))\n",
    "print(\"Number_of_Casualties std deviation = {}\".format(nc_std_dev))\n",
    "deviations = np.abs(nc_column - nc_mean)\n",
    "df_accidents_05_14 = df_accidents_05_14[nc_column <= np.round(nc_mean + 3 * nc_std_dev)]\n",
    "\n",
    "# Plot Number_of_Casualties again\n",
    "plt.scatter(df_accidents_05_14.index, df_accidents_05_14['Number_of_Casualties'])\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"Number_of_Casualties Updated\")\n",
    "print(\"df_accidents_05_14 new shape is: {}\".format(df_accidents_05_14.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-software",
   "metadata": {},
   "source": [
    "### *'Date'*, *'Day_of_Week'* and *'Time'* Columns\n",
    "These columns locate in time the presence of the accident. Start by determining if they have missing values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in Date: {}\".format(df_accidents_05_14[df_accidents_05_14['Date'].isna()].shape[0]))\n",
    "print(\"Missing values in Day_of_Week: {}\".format(df_accidents_05_14[df_accidents_05_14['Day_of_Week'].isna()].shape[0]))\n",
    "print(\"Missing values in Time: {}\".format(df_accidents_05_14[df_accidents_05_14['Time'].isna()].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values for Time\n",
    "df_accidents_05_14.dropna(subset=['Time'], inplace=True)\n",
    "print(\"df_accidents_05_14 new shape is: {}\".format(df_accidents_05_14.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-connecticut",
   "metadata": {},
   "source": [
    "Since the *'Date'* as it is will not represent anything for a mathematical model, we will break it into 3 separate columns: *'Month_Day'*, *'Month'* and *'Year'*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by converting Date column to datetime type\n",
    "df_accidents_05_14['Date'] = pd.to_datetime(df_accidents_05_14['Date'], format='%d/%m/%Y')\n",
    "\n",
    "# Create new separate columns\n",
    "df_accidents_05_14['Month_Day'] = df_accidents_05_14['Date'].dt.day\n",
    "df_accidents_05_14['Month'] = df_accidents_05_14['Date'].dt.month\n",
    "df_accidents_05_14['Year'] = df_accidents_05_14['Date'].dt.year\n",
    "\n",
    "# Drop 'Date' column\n",
    "df_accidents_05_14 = df_accidents_05_14.drop(columns=['Date'])\n",
    "print(\"df_accidents_05_14 new shape is: {}\".format(df_accidents_05_14.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-collector",
   "metadata": {},
   "source": [
    "Since the *'Time'* as it is will not represent anything for a mathematical model, we will break it into 2 separate columns: *'Hour'* and *'Minute'*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by converting Time column to datetime type\n",
    "df_accidents_05_14['Time'] = pd.to_datetime(df_accidents_05_14['Time'], format='%H:%M')\n",
    "\n",
    "# Create new separate columns\n",
    "df_accidents_05_14['Hour'] = df_accidents_05_14['Time'].dt.hour\n",
    "df_accidents_05_14['Minute'] = df_accidents_05_14['Time'].dt.minute\n",
    "\n",
    "# Drop 'Time' column\n",
    "df_accidents_05_14 = df_accidents_05_14.drop(columns=['Time'])\n",
    "print(\"df_accidents_05_14 new shape is: {}\".format(df_accidents_05_14.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-decrease",
   "metadata": {},
   "source": [
    "### 'Local_Authority_(District)' Column\n",
    "This column represents the identifier for the district authority. As we can see below this column does not have missing items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-lease",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14[df_accidents_05_14['Local_Authority_(District)'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-polls",
   "metadata": {},
   "source": [
    "From the plot below, we do not see any clear outlier. We will just rename the column to avoid the parenthesis in the name.\n",
    "\n",
    "Since these are categories, one option here may have been converting to separate columns using one-hot encodings, however in this case we are dealing with almost 1000 types which makes it inconvenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14 = df_accidents_05_14.rename(columns={'Local_Authority_(District)': 'Local_Authority_District'})\n",
    "\n",
    "plt.scatter(df_accidents_05_14.index, df_accidents_05_14['Local_Authority_District'])\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"Local_Authority_District\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-carnival",
   "metadata": {},
   "source": [
    "### 'Local_Authority_(Highway)' Column\n",
    "This column represents the identifier for the highway authority. As we can see below this column does not have missing items. Also rename the column to avoid the parenthesis in the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14 = df_accidents_05_14.rename(columns={'Local_Authority_(Highway)': 'Local_Authority_Highway'})\n",
    "df_accidents_05_14[df_accidents_05_14['Local_Authority_Highway'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-processing",
   "metadata": {},
   "source": [
    "Since this column also represents categories, we will convert it into this type for simpler manipulation.The same as before, one-hot encodings is not as practical since we are dealing with more than 200 cateogires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14['Local_Authority_Highway'] = df_accidents_05_14['Local_Authority_Highway'].astype('category')\n",
    "# If we desired to have the codes as numbers\n",
    "# df_accidents_05_14['Local_Authority_Highway'] = df_accidents_05_14['Local_Authority_Highway'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_accidents_05_14.index, df_accidents_05_14['Local_Authority_Highway'].cat.codes)\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"Local_Authority_Highway (code)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-intranet",
   "metadata": {},
   "source": [
    "### *'1st_Road_Class'* and *'2nd_Road_Class'* Columns\n",
    "From the dataset documentation, it is not very clear on what these columns represent. As it can be seen below, they does not have any missing values and contain 6 potential values. It may be wroth applying one-hot encodings to these columns after knowing what each of the categories represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-proposition",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Missing values in 1st_Road_Class: {}\".format(df_accidents_05_14[df_accidents_05_14['1st_Road_Class'].isna()].shape[0]))\n",
    "print(\"Missing values in 2nd_Road_Class: {}\".format(df_accidents_05_14[df_accidents_05_14['2nd_Road_Class'].isna()].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14.hist(column='1st_Road_Class', bins=6)\n",
    "df_accidents_05_14.hist(column='2nd_Road_Class', bins=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-regular",
   "metadata": {},
   "source": [
    "### *'1st_Road_Number'* and *'2nd_Road_Number'* Columns\n",
    "First and second number for the road where the accident happened. There are not missing items and we will keep them as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in 1st_Road_Number: {}\".format(df_accidents_05_14[df_accidents_05_14['1st_Road_Number'].isna()].shape[0]))\n",
    "print(\"Missing values in 2nd_Road_Number: {}\".format(df_accidents_05_14[df_accidents_05_14['2nd_Road_Number'].isna()].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_accidents_05_14.index, df_accidents_05_14['1st_Road_Number'])\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"1st_Road_Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_accidents_05_14.index, df_accidents_05_14['2nd_Road_Number'])\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"2nd_Road_Number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-tribune",
   "metadata": {},
   "source": [
    "### *'Road_Type'* Column\n",
    "This column indicates the type of road where the accident was presented. As it can be seen below, we have no missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14[df_accidents_05_14['Road_Type'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-operation",
   "metadata": {},
   "source": [
    "From the plot below, we can observe that there are five type of roads defined and an Unknown category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot by index\n",
    "plt.scatter(df_accidents_05_14.index, df_accidents_05_14['Road_Type'])\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"Road_Type\")\n",
    "\n",
    "# Count by each type\n",
    "df_accidents_05_14['Road_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-ensemble",
   "metadata": {},
   "source": [
    "This is a good oportunity to apply one-hot encodings over this column. We can only keep the the rows with the values with more instances and delete the rows with the *'Unknown'* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new columns from one-hot encodings and drop Unknown\n",
    "one_hot = pd.get_dummies(df_accidents_05_14['Road_Type'])\n",
    "one_hot.drop(columns=['Unknown'], inplace=True)\n",
    "\n",
    "# Rename Columns of one-hot\n",
    "one_hot.rename(columns={'Dual carriageway': 'Road_Dual_Carriageway',\n",
    "                       'One way street': 'Road_One_Way_Street',\n",
    "                       'Roundabout': 'Road_Roundabout',\n",
    "                       'Single carriageway': 'Road_Single_Carriageway',\n",
    "                       'Slip road': 'Road_Slip'}, \n",
    "               inplace=True)\n",
    "\n",
    "# Add one-hot to original df and drop Road_Type column\n",
    "df_accidents_05_14 = df_accidents_05_14.join(one_hot)\n",
    "df_accidents_05_14.drop(columns=['Road_Type'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-border",
   "metadata": {},
   "source": [
    "### *'Speed_limit'* Column\n",
    "Speed limit for the road where the accident happened. There are not missing items and we will keep the data as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14[df_accidents_05_14['1st_Road_Number'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14.hist(column='Speed_limit', bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-sequence",
   "metadata": {},
   "source": [
    "### *'Junction_Detail'* and *'Junction_Control'* Columns\n",
    "As it can be seen below, there is a very high number of instances which do not have a value for these columns. Considering this, we will go ahead and get them completely deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in Junction_Detail: {}\".format(df_accidents_05_14[df_accidents_05_14['Junction_Detail'].isna()].shape[0]))\n",
    "print(\"Missing values in Junction_Control: {}\".format(df_accidents_05_14[df_accidents_05_14['Junction_Control'].isna()].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14.drop(columns=['Junction_Detail', 'Junction_Control'], inplace=True)\n",
    "print(\"df_accidents_05_14 new shape is: {}\".format(df_accidents_05_14.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-spanish",
   "metadata": {},
   "source": [
    "### *'Pedestrian_Crossing-Human_Control'* Column\n",
    "\n",
    "This column contains a string indicating if there is human control for pedestrial crossing close to the site of the accident. Below can see that there are a few missing items that we proceed to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14 = df_accidents_05_14.rename(columns={'Pedestrian_Crossing-Human_Control': 'Pedestrian_Crossing_Human_Control'})\n",
    "print(\"Missing values in Pedestrian_Crossing_Human_Control: {}\".format(df_accidents_05_14[df_accidents_05_14['Pedestrian_Crossing_Human_Control'].isna()].shape[0]))\n",
    "df_accidents_05_14.dropna(subset=['Pedestrian_Crossing_Human_Control'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-restriction",
   "metadata": {},
   "source": [
    "Below we identify the type of classes for this column. As it can be seen below there are only 3 type of classes, which means we can apply one-hot encodings for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count by each type\n",
    "df_accidents_05_14['Pedestrian_Crossing_Human_Control'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new columns from one-hot encodings\n",
    "one_hot = pd.get_dummies(df_accidents_05_14['Pedestrian_Crossing_Human_Control'])\n",
    "\n",
    "# Rename Columns of one-hot\n",
    "one_hot.rename(columns={'None within 50 metres': 'Pedestrian_Crossing_Control_None',\n",
    "                       'Control by other authorised person': 'Pedestrian_Crossing_Control_Authorized_Person',\n",
    "                       'Control by school crossing patrol': 'Pedestrian_Crossing_Control_School_Patrol'}, \n",
    "               inplace=True)\n",
    "\n",
    "# Add one-hot to original df and drop Pedestrian_Crossing_Human_Control column\n",
    "df_accidents_05_14 = df_accidents_05_14.join(one_hot)\n",
    "df_accidents_05_14.drop(columns=['Pedestrian_Crossing_Human_Control'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-administrator",
   "metadata": {},
   "source": [
    "### *'Pedestrian_Crossing-Physical_Facilities'* Column\n",
    "\n",
    "This column contains a string indicating if there is a facility for pedestrial crossing close to the site of the accident. Below can see that there are a few missing items that we proceed to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_05_14 = df_accidents_05_14.rename(columns={'Pedestrian_Crossing-Physical_Facilities': 'Pedestrian_Crossing_Physical_Facilities'})\n",
    "print(\"Missing values in Pedestrian_Crossing_Physical_Facilities: {}\".format(df_accidents_05_14[df_accidents_05_14['Pedestrian_Crossing_Physical_Facilities'].isna()].shape[0]))\n",
    "df_accidents_05_14.dropna(subset=['Pedestrian_Crossing_Physical_Facilities'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-berry",
   "metadata": {},
   "source": [
    "Below we identify the type of classes for this column. As it can be seen below there are only 6 types of classes, which means we can apply one-hot encodings for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count by each type\n",
    "df_accidents_05_14['Pedestrian_Crossing_Physical_Facilities'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new columns from one-hot encodings\n",
    "one_hot = pd.get_dummies(df_accidents_05_14['Pedestrian_Crossing_Physical_Facilities'])\n",
    "\n",
    "# Rename Columns of one-hot\n",
    "one_hot.rename(columns={'No physical crossing within 50 meters': 'Pedestrian_Crossing_Fac_None',\n",
    "                       'Pedestrian phase at traffic signal junction': 'Pedestrian_Crossing_Fac_Ped_Phase_at_Traffic',\n",
    "                       'non-junction pedestrian crossing': 'Pedestrian_Crossing_Fac_NonJun_Crossing',\n",
    "                       'Zebra crossing': 'Pedestrian_Crossing_Fac_Zebra',\n",
    "                       'Central refuge': 'Pedestrian_Crossing_Fac_Central_Refuge',\n",
    "                       'Footbridge or subway': 'Pedestrian_Crossing_Fac_Footbridge'}, \n",
    "               inplace=True)\n",
    "\n",
    "# Add one-hot to original df and drop Pedestrian_Crossing_Physical_Facilities column\n",
    "df_accidents_05_14 = df_accidents_05_14.join(one_hot)\n",
    "df_accidents_05_14.drop(columns=['Pedestrian_Crossing_Physical_Facilities'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-transportation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_accidents_05_14.dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
